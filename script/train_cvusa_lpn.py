import os
#
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '3'

from spatial_net import *
from input_data_cvusa import InputData
import tensorflow as tf
import numpy as np
import argparse

parser = argparse.ArgumentParser(description='TensorFlow implementation.')

parser.add_argument('--network_type',              type=str,   help='network type',      default='multi_loss_LPN_AWARE_8')
parser.add_argument('--polar',                     type=int,   help='polar',             default=1)
parser.add_argument('--start_epoch',               type=int,   help='train from epoch',  default=0)
parser.add_argument('--lr_decay',                  action='store_true',   help='decay learning rate')
parser.add_argument('--multi_loss',                action='store_true',   help='use multiple loss')
args = parser.parse_args()

# --------------  configuration parameters  -------------- #
# the type of network to be used: "CVM-NET-I" or "CVM-NET-II"
network_type = args.network_type
print('network_type:----------------------', network_type)
polar = args.polar
start_epoch = args.start_epoch
lr_decay = args.lr_decay
multi_loss = args.multi_loss
data_type = 'CVUSA'

batch_size = 32
is_training = True
loss_weight = 10.0
number_of_epoch = 100

#add lr rate decay
if lr_decay:
    boundaries = [60]
    learning_rate_vals = [1e-5, 3e-6]
    print('-----------adapt learning rate decay-----------')
# no lr decay
else:
    learning_rate_val = 1e-5
keep_prob_val = 0.8
# -------------------------------------------------------- #

print('learning rate:-----------------------', learning_rate_val)

def validate(grd_descriptor, sat_descriptor):
    accuracy = 0.0
    data_amount = 0.0
    dist_array = 2 - 2 * np.matmul(sat_descriptor, np.transpose(grd_descriptor))
    for i in range(dist_array.shape[0]):
        gt_dist = dist_array[i, i]
        prediction = np.sum(dist_array[:, i] < gt_dist)
        if prediction < 1:
            accuracy += 1.0
        data_amount += 1.0
    accuracy /= data_amount

    return accuracy


def compute_loss(sat_global, grd_global, batch_hard_count=0):
    '''
    Compute the weighted soft-margin triplet loss
    :param sat_global: the satellite image global descriptor
    :param grd_global: the ground image global descriptor
    :param batch_hard_count: the number of top hard pairs within a batch. If 0, no in-batch hard negative mining
    :return: the losse
    '''
    with tf.name_scope('weighted_soft_margin_triplet_loss'):

        dist_array = 2 - 2 * tf.matmul(sat_global, grd_global, transpose_b=True)

        pos_dist = tf.diag_part(dist_array)
        if batch_hard_count == 0:
            pair_n = batch_size * (batch_size - 1.0)

            # ground to satellite
            triplet_dist_g2s = pos_dist - dist_array
            loss_g2s = tf.reduce_sum(tf.log(1 + tf.exp(triplet_dist_g2s * loss_weight))) / pair_n

            # satellite to ground
            triplet_dist_s2g = tf.expand_dims(pos_dist, 1) - dist_array
            loss_s2g = tf.reduce_sum(tf.log(1 + tf.exp(triplet_dist_s2g * loss_weight))) / pair_n

            loss = (loss_g2s + loss_s2g) / 2.0
        else:
            # ground to satellite
            triplet_dist_g2s = pos_dist - dist_array
            triplet_dist_g2s = tf.log(1 + tf.exp(triplet_dist_g2s * loss_weight))
            top_k_g2s, _ = tf.nn.top_k(tf.transpose(triplet_dist_g2s), batch_hard_count)
            loss_g2s = tf.reduce_mean(top_k_g2s)

            # satellite to ground
            triplet_dist_s2g = tf.expand_dims(pos_dist, 1) - dist_array
            triplet_dist_s2g = tf.log(1 + tf.exp(triplet_dist_s2g * loss_weight))
            top_k_s2g, _ = tf.nn.top_k(triplet_dist_s2g, batch_hard_count)
            loss_s2g = tf.reduce_mean(top_k_s2g)

            loss = (loss_g2s + loss_s2g) / 2.0

    return loss

def compute_multi_loss(sat_global, grd_global):
    batch, d, channel = sat_global.get_shape().as_list()
    loss = compute_loss(sat_global[:,0,:], grd_global[:,0,:])
    for i in range(d-1):
        loss += compute_loss(sat_global[:,i+1,:], grd_global[:,i+1,:])
    loss = loss / d
    return loss

def train(start_epoch=0):

    # import data
    input_data = InputData(polar)

    # define placeholders
    if polar:
        sat_x = tf.placeholder(tf.float32, [None, 112, 616, 3], name='sat_x')
    else:
        sat_x = tf.placeholder(tf.float32, [None, 256, 256, 3], name='sat_x')

    grd_x = tf.placeholder(tf.float32, [None, 112, 616, 3], name='grd_x')

    keep_prob = tf.placeholder(tf.float32)
    learning_rate = tf.placeholder(tf.float32)

    # build model
    dimension = int(network_type[-1])
    if 'LPN_AWARE' in network_type: # SAFA + LPN
        print('-----------------lpn aware net--------------------------')
        sat_global, grd_global = LPN_AWARE(sat_x, grd_x, keep_prob, dimension, is_training, multi_loss)
    else: # SAFA
        sat_global, grd_global = SAFA(sat_x, grd_x, keep_prob, dimension, is_training)

    if multi_loss: 
        batch, d, out_channel = sat_global.get_shape().as_list()
        sat_global_descriptor = np.zeros([input_data.get_test_dataset_size(), d*out_channel])
        grd_global_descriptor = np.zeros([input_data.get_test_dataset_size(), d*out_channel])
        loss = compute_multi_loss(sat_global, grd_global)

    else:
        out_channel = sat_global.get_shape().as_list()[-1]
        sat_global_descriptor = np.zeros([input_data.get_test_dataset_size(), out_channel])
        grd_global_descriptor = np.zeros([input_data.get_test_dataset_size(), out_channel])

        loss = compute_loss(sat_global, grd_global)


    # set training
    global_step = tf.Variable(0, trainable=False)
    with tf.device('/gpu:0'):
        with tf.name_scope('train'):
            train_step = tf.train.AdamOptimizer(learning_rate, 0.9, 0.999).minimize(loss, global_step=global_step)

    print('setting saver...')
    saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)
    print('setting saver done...')

    # run model
    print('run model...')
    config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)
    config.gpu_options.allow_growth = True
    config.gpu_options.per_process_gpu_memory_fraction = 0.9
    print('open session ...')
    with tf.Session(config=config) as sess:
        print('initialize...')
        sess.run(tf.global_variables_initializer())

        print('load model...')

        if start_epoch == 0:
            if 'LPN_AWARE' in network_type:
                restore_var = [v for v in tf.global_variables()]
                variables_to_resotre = [v for v in restore_var if v.name.split('/')[0] in ['VGG_grd','VGG_sat']]
                saver_vgg = tf.train.Saver(var_list=variables_to_resotre)
                for i in range(len(variables_to_resotre)):
                    print(variables_to_resotre[i])
                load_model_path = '~/SAFA-LPN/Model/Initialize/initial_model.ckpt'
                saver_vgg.restore(sess, load_model_path)
            else:        
                load_model_path = '~/SAFA-LPN/Model/Initialize/initial_model.ckpt'
                saver.restore(sess, load_model_path)
        else:

            load_model_path = '~/SAFA-LPN/Model/' + data_type + '/' + network_type \
                          + '/polar_' + str(polar) + '/' + str(start_epoch - 1) + '/model.ckpt'

            saver.restore(sess, load_model_path)

        print("   Model loaded from: %s" % load_model_path)
        print('load model...FINISHED')

        # Train
        for epoch in range(start_epoch, start_epoch + number_of_epoch):
            iter = 0
            while True:
                # train
                batch_sat, batch_grd = input_data.next_pair_batch(batch_size)
                if batch_sat is None:
                    break

                global_step_val = tf.train.global_step(sess, global_step)

                # get current lr
                if lr_decay:
                    lr_val = tf.train.piecewise_constant(epoch, boundaries=boundaries, values=learning_rate_vals)
                    lr = sess.run(lr_val)
                    feed_dict = {sat_x: batch_sat, grd_x: batch_grd,
                                learning_rate: lr, keep_prob: keep_prob_val}
                # no change lr
                else:
                    feed_dict = {sat_x: batch_sat, grd_x: batch_grd,
                                learning_rate: learning_rate_val, keep_prob: keep_prob_val}
                if iter % 20 == 0:
                    _, loss_val = sess.run([train_step, loss], feed_dict=feed_dict)
                    if lr_decay:
                        print('global %d, epoch %d, iter %d, lr : %.5f, loss : %.4f ' % (global_step_val, epoch, iter, lr, loss_val))
                    else:
                        print('global %d, epoch %d, iter %d, loss : %.4f ' % (global_step_val, epoch, iter, loss_val))
                else:
                    sess.run(train_step, feed_dict=feed_dict)

                iter += 1

            model_dir = '~/SAFA_LPN/Model/' + data_type + '/' + network_type \
                        + '/polar_' + str(polar) + '/' + str(epoch) + '/'

            if not os.path.exists(model_dir):
                os.makedirs(model_dir)
            save_path = saver.save(sess, model_dir + 'model.ckpt')
            print("Model saved in file: %s" % save_path)

            # ---------------------- validation ----------------------

            print('validate...')
            print('   compute global descriptors')
            input_data.reset_scan()

            val_i = 0
            while True:
                # print('      progress %d' % val_i)
                batch_sat, batch_grd = input_data.next_batch_scan(batch_size) 
                    
                if batch_sat is None:
                    break
                feed_dict = {sat_x: batch_sat, grd_x: batch_grd, keep_prob: 1.0}
                sat_global_val, grd_global_val = \
                    sess.run([sat_global, grd_global], feed_dict=feed_dict)
                if multi_loss:              #out shape:batch*d*channel
                    batch, d, channel = sat_global_val.shape
                    sat_global_val = sat_global_val.reshape(-1, d*channel)
                    grd_global_val = grd_global_val.reshape(-1, d*channel)
                sat_global_descriptor[val_i: val_i + sat_global_val.shape[0], :] = sat_global_val
                grd_global_descriptor[val_i: val_i + grd_global_val.shape[0], :] = grd_global_val
                val_i += sat_global_val.shape[0]

            print('   compute accuracy')
            val_accuracy = validate(grd_global_descriptor, sat_global_descriptor)
            print('   %d: top-1 accuracy = %.1f' % (epoch, val_accuracy * 100.0))
            file = '~/SAFA_LPN/Result/' + data_type + '/' + str(network_type) +'_polar_' + str(polar) + '_accuracy.txt'
            with open(file, 'a') as file:
                file.write(str(epoch) + ' ' + str(iter) + ' : ' + str(val_accuracy) + '\n')


if __name__ == '__main__':
    train(start_epoch)
